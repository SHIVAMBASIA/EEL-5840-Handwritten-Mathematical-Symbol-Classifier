{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9032, 1, 150, 150)\n",
      "Training images available: 7225\n",
      "Validation images available:  1807\n",
      "epoch:  0  batch:  100  loss: 1.71529126 accuracy:  16.800%\n",
      "epoch:  0  batch:  200  loss: 1.38660383 accuracy:  29.450%\n",
      "epoch:  0  batch:  300  loss: 1.14351439 accuracy:  38.233%\n",
      "epoch:  0  batch:  400  loss: 1.82453287 accuracy:  44.175%\n",
      "epoch:  0  batch:  500  loss: 0.95740682 accuracy:  48.700%\n",
      "epoch:  0  batch:  600  loss: 0.51167583 accuracy:  52.250%\n",
      "epoch:  0  batch:  700  loss: 0.68143624 accuracy:  55.114%\n",
      "Overall train accuracy after 0th epoch: 55.5709342956543\n",
      "Overall validation accuracy after 0th epoch: 72.55118560791016\n",
      "epoch:  1  batch:  100  loss: 0.71882904 accuracy:  78.800%\n",
      "epoch:  1  batch:  200  loss: 0.62981093 accuracy:  77.300%\n",
      "epoch:  1  batch:  300  loss: 0.25593212 accuracy:  77.567%\n",
      "epoch:  1  batch:  400  loss: 0.57822484 accuracy:  78.800%\n",
      "epoch:  1  batch:  500  loss: 0.43073297 accuracy:  79.100%\n",
      "epoch:  1  batch:  600  loss: 0.25867540 accuracy:  79.500%\n",
      "epoch:  1  batch:  700  loss: 0.96215695 accuracy:  79.657%\n",
      "Overall train accuracy after 1th epoch: 79.72318267822266\n",
      "Overall validation accuracy after 1th epoch: 79.91145324707031\n",
      "epoch:  2  batch:  100  loss: 0.79698545 accuracy:  86.700%\n",
      "epoch:  2  batch:  200  loss: 0.49785218 accuracy:  85.450%\n",
      "epoch:  2  batch:  300  loss: 0.01522287 accuracy:  85.467%\n",
      "epoch:  2  batch:  400  loss: 0.45418167 accuracy:  85.775%\n",
      "epoch:  2  batch:  500  loss: 0.25428370 accuracy:  85.760%\n",
      "epoch:  2  batch:  600  loss: 0.47685632 accuracy:  85.783%\n",
      "epoch:  2  batch:  700  loss: 0.58714855 accuracy:  85.729%\n",
      "Overall train accuracy after 2th epoch: 85.74394989013672\n",
      "Overall validation accuracy after 2th epoch: 81.9037094116211\n",
      "epoch:  3  batch:  100  loss: 0.67726409 accuracy:  90.000%\n",
      "epoch:  3  batch:  200  loss: 1.49864912 accuracy:  90.750%\n",
      "epoch:  3  batch:  300  loss: 0.18928313 accuracy:  90.300%\n",
      "epoch:  3  batch:  400  loss: 1.02960563 accuracy:  90.400%\n",
      "epoch:  3  batch:  500  loss: 0.04080332 accuracy:  90.340%\n",
      "epoch:  3  batch:  600  loss: 0.19857998 accuracy:  89.967%\n",
      "epoch:  3  batch:  700  loss: 0.27640131 accuracy:  89.729%\n",
      "Overall train accuracy after 3th epoch: 89.73011016845703\n",
      "Overall validation accuracy after 3th epoch: 84.28334045410156\n",
      "epoch:  4  batch:  100  loss: 0.12495436 accuracy:  93.000%\n",
      "epoch:  4  batch:  200  loss: 0.00727254 accuracy:  93.100%\n",
      "epoch:  4  batch:  300  loss: 0.04257942 accuracy:  93.633%\n",
      "epoch:  4  batch:  400  loss: 0.04727294 accuracy:  93.600%\n",
      "epoch:  4  batch:  500  loss: 0.03765535 accuracy:  93.000%\n",
      "epoch:  4  batch:  600  loss: 0.24705060 accuracy:  92.783%\n",
      "epoch:  4  batch:  700  loss: 0.05444510 accuracy:  92.671%\n",
      "Overall train accuracy after 4th epoch: 92.69204711914062\n",
      "Overall validation accuracy after 4th epoch: 82.512451171875\n",
      "epoch:  5  batch:  100  loss: 0.50598568 accuracy:  95.700%\n",
      "epoch:  5  batch:  200  loss: 0.00318161 accuracy:  96.000%\n",
      "epoch:  5  batch:  300  loss: 0.03249322 accuracy:  95.433%\n",
      "epoch:  5  batch:  400  loss: 0.20500703 accuracy:  95.375%\n",
      "epoch:  5  batch:  500  loss: 0.02740680 accuracy:  94.900%\n",
      "epoch:  5  batch:  600  loss: 0.05421711 accuracy:  94.633%\n",
      "epoch:  5  batch:  700  loss: 0.01589769 accuracy:  94.600%\n",
      "Overall train accuracy after 5th epoch: 94.50519561767578\n",
      "Overall validation accuracy after 5th epoch: 82.62313079833984\n",
      "epoch:  6  batch:  100  loss: 0.00281903 accuracy:  95.200%\n",
      "epoch:  6  batch:  200  loss: 0.00183786 accuracy:  95.250%\n",
      "epoch:  6  batch:  300  loss: 0.22032921 accuracy:  95.000%\n",
      "epoch:  6  batch:  400  loss: 0.02200981 accuracy:  94.850%\n",
      "epoch:  6  batch:  500  loss: 0.27833977 accuracy:  94.960%\n",
      "epoch:  6  batch:  600  loss: 0.00052878 accuracy:  95.100%\n",
      "epoch:  6  batch:  700  loss: 0.09885895 accuracy:  95.186%\n",
      "Overall train accuracy after 6th epoch: 95.15571594238281\n",
      "Overall validation accuracy after 6th epoch: 83.1765365600586\n",
      "epoch:  7  batch:  100  loss: 0.00338915 accuracy:  95.600%\n",
      "epoch:  7  batch:  200  loss: 0.15102647 accuracy:  95.900%\n",
      "epoch:  7  batch:  300  loss: 0.04220139 accuracy:  96.267%\n",
      "epoch:  7  batch:  400  loss: 0.02367832 accuracy:  96.350%\n",
      "epoch:  7  batch:  500  loss: 0.28546229 accuracy:  96.320%\n",
      "epoch:  7  batch:  600  loss: 0.02933961 accuracy:  96.333%\n",
      "epoch:  7  batch:  700  loss: 0.25666118 accuracy:  96.400%\n",
      "Overall train accuracy after 7th epoch: 96.4013900756836\n",
      "Overall validation accuracy after 7th epoch: 83.50857543945312\n",
      "epoch:  8  batch:  100  loss: 0.01098348 accuracy:  96.700%\n",
      "epoch:  8  batch:  200  loss: 0.00143453 accuracy:  97.500%\n",
      "epoch:  8  batch:  300  loss: 0.00072775 accuracy:  97.367%\n",
      "epoch:  8  batch:  400  loss: 0.00014028 accuracy:  97.250%\n",
      "epoch:  8  batch:  500  loss: 0.00024091 accuracy:  97.180%\n",
      "epoch:  8  batch:  600  loss: 0.13580336 accuracy:  97.150%\n",
      "epoch:  8  batch:  700  loss: 0.00012455 accuracy:  97.000%\n",
      "Overall train accuracy after 8th epoch: 96.99654388427734\n",
      "Overall validation accuracy after 8th epoch: 82.78915405273438\n",
      "epoch:  9  batch:  100  loss: 0.00384400 accuracy:  97.600%\n",
      "epoch:  9  batch:  200  loss: 0.04463417 accuracy:  98.000%\n",
      "epoch:  9  batch:  300  loss: 0.26918259 accuracy:  97.533%\n",
      "epoch:  9  batch:  400  loss: 0.00604931 accuracy:  97.600%\n",
      "epoch:  9  batch:  500  loss: 0.05092020 accuracy:  97.740%\n",
      "epoch:  9  batch:  600  loss: 0.00018085 accuracy:  97.750%\n",
      "epoch:  9  batch:  700  loss: 0.11342235 accuracy:  97.457%\n",
      "Overall train accuracy after 9th epoch: 97.4117660522461\n",
      "Overall validation accuracy after 9th epoch: 83.23187255859375\n",
      "[[858   4   3   1   3  15   5   1   9   4   2]\n",
      " [  3 876   2   2   2   4   0   0   5   1   0]\n",
      " [  5   4 870   6   1   0   4   0   3   0   0]\n",
      " [  4   4  11 845  13   0   0   0   3   1   7]\n",
      " [ 10   6   1  16 834   5   5   1   0   2   0]\n",
      " [ 22   5   4   1   0 852   3   1   7   4   0]\n",
      " [ 15   1   1   0   3   6 885   2   2   3   2]\n",
      " [  3   1   2   2   1   0   5 790  47   0   0]\n",
      " [ 17   1   5   1   3   4   2  61 825   2   2]\n",
      " [ 14   2   1   2   6   8   8   0   3 848   2]\n",
      " [  6   0   1   4   1   3   5   2   3   0  59]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.92       905\n",
      "           1       0.97      0.98      0.97       895\n",
      "           2       0.97      0.97      0.97       893\n",
      "           3       0.96      0.95      0.96       888\n",
      "           4       0.96      0.95      0.95       880\n",
      "           5       0.95      0.95      0.95       899\n",
      "           6       0.96      0.96      0.96       920\n",
      "           7       0.92      0.93      0.92       851\n",
      "           8       0.91      0.89      0.90       923\n",
      "           9       0.98      0.95      0.96       894\n",
      "          10       0.80      0.70      0.75        84\n",
      "\n",
      "    accuracy                           0.95      9032\n",
      "   macro avg       0.93      0.93      0.93      9032\n",
      "weighted avg       0.95      0.95      0.95      9032\n",
      "\n",
      "\n",
      "Duration for training: 442 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "######################### inputs ##################################\n",
    "\n",
    "# The inputs can be passed as npy files here\n",
    "\n",
    "images = np.load('data_train.npy')         # data set X.        Expected dimensions(X,300,300)\n",
    "labels = np.load('t_train_corrected.npy')        # desired output y.  Expected dimensions(y,)\n",
    "labels[labels == -1] = 10\n",
    "\n",
    "########################## Model ####################################\n",
    "\n",
    "class ConvolutionalNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 11, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(11, 20, 3, 1)\n",
    "        self.conv3 = nn.Conv2d(20, 30, 3, 1)\n",
    "        self.fc1 = nn.Linear(17*17*30, 200)\n",
    "        self.fc2 = nn.Linear(200, 100)\n",
    "        self.fc3 = nn.Linear(100, 25)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = F.relu(self.conv1(X))\n",
    "        X = F.avg_pool2d(X, 2)\n",
    "        X = F.relu(self.conv2(X))\n",
    "        X = F.avg_pool2d(X, 2)\n",
    "        X = F.relu(self.conv3(X))\n",
    "        X = F.avg_pool2d(X, 2)\n",
    "        X = X.view(-1, 17*17*30)\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = F.relu(self.fc2(X))\n",
    "        X = self.fc3(X)\n",
    "        return F.log_softmax(X, dim=1)\n",
    "\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    \n",
    "torch.manual_seed(101)\n",
    "CNNmodel = ConvolutionalNetwork()#.to(device)\n",
    "criterion = nn.CrossEntropyLoss()#.to(device)\n",
    "optimizer = torch.optim.Adam(CNNmodel.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "########################## function definition ####################################\n",
    "\n",
    "def normalize_data(X):\n",
    "    mu = np.mean(X)\n",
    "    std = np.std(X)\n",
    "    return ((X-mu)/std)\n",
    "\n",
    "# Trains the model for input images and labels. Prints the accuracy on the training and validation datasets. Also prints the final confusion matrix.\n",
    "def train(images, labels):\n",
    "    #images, labels = images.to(device), labels.to(device)\n",
    "    # Set parameters\n",
    "    epochs = 10\n",
    "    \n",
    "    batch_size = 10\n",
    "    max_train_batches = 3000\n",
    "    max_val_batches = 3000\n",
    "    \n",
    "    images = 255 - images\n",
    "\n",
    "    images=images.T\n",
    "    #X = images.T\n",
    "    X_train_resized = np.array([cv2.resize(x.reshape(300,300),(150,150)).reshape(150*150) for x in images])\n",
    "    #X_train_resized.shape\n",
    "    X_train_resized = X_train_resized.reshape(len(X_train_resized),1,150,150)\n",
    "    print(X_train_resized.shape)\n",
    "    #X_train_resized, labels = X_train_resized.to(device), labels.to(device)\n",
    "    \n",
    "    #Set test_size = 0, if validation set is not needed.\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_resized, labels, test_size=0.20, random_state=42)\n",
    "    \n",
    "    \n",
    "    # Normalization of data\n",
    "    X_train = normalize_data(X_train)\n",
    "    X_val = normalize_data(X_val)\n",
    "    \n",
    "    \n",
    "    # Converting input numpy array to pytorch tensors\n",
    "    X_trainTensor = torch.Tensor(X_train)\n",
    "    X_valTensor = torch.Tensor(X_val)\n",
    "    y_trainTensor = torch.Tensor(y_train)\n",
    "    y_valTensor = torch.Tensor(y_val)\n",
    "    \n",
    "    y_trainTensor = y_trainTensor.type(torch.LongTensor)\n",
    "    y_valTensor = y_valTensor.type(torch.LongTensor)\n",
    "    \n",
    "    \n",
    "    # Create training dataset and loader\n",
    "    train_data = TensorDataset(X_trainTensor,y_trainTensor) \n",
    "    train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True) \n",
    "    \n",
    "    # Create validation dataset and loader\n",
    "    val_data = TensorDataset(X_valTensor,y_valTensor) \n",
    "    val_loader = DataLoader(val_data,batch_size=batch_size,shuffle=True) \n",
    "    \n",
    "    \n",
    "    print(f'Training images available: {len(train_data)}')\n",
    "    print(f'Validation images available:  {len(val_data)}')\n",
    "\n",
    "    \n",
    "    # Training start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_correct = []\n",
    "    val_correct = []\n",
    "    final_predicted = []\n",
    "    final_actual = []\n",
    "    \n",
    "    # For each Epoch\n",
    "    for i in range(epochs):\n",
    "        trn_corr = 0\n",
    "        val_corr = 0\n",
    "        \n",
    "        # For each training batch\n",
    "        for b, (X_train, y_train) in enumerate(train_loader):\n",
    "            \n",
    "            # Limit the number of batches\n",
    "            if b == max_train_batches:\n",
    "                break\n",
    "            b += 1\n",
    "            \n",
    "            # Apply the model\n",
    "            y_pred = CNNmodel(X_train)\n",
    "            loss = criterion(y_pred, y_train)\n",
    "     \n",
    "            # Tally the number of correct predictions\n",
    "            predicted = torch.max(y_pred.data, 1)[1]\n",
    "            trn_corr += (predicted == y_train).sum()\n",
    "            \n",
    "            # Store predictions of final epoch for confusion matrix\n",
    "            if(i == epochs-1):\n",
    "                final_predicted += predicted\n",
    "                final_actual += y_train\n",
    "            \n",
    "            # Update parameters\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            # Print interim results\n",
    "            if b%100 == 0:\n",
    "                print(f'epoch: {i:2}  batch: {b:4}  loss: {loss.item():10.8f} accuracy: {trn_corr.item()*100/(10*b):7.3f}%')\n",
    "    \n",
    "        train_losses.append(loss)\n",
    "        train_correct.append(trn_corr)\n",
    "    \n",
    "        # For each validation batch\n",
    "        with torch.no_grad():\n",
    "            for b, (X_val, y_val) in enumerate(val_loader):\n",
    "                \n",
    "                # Limit the number of batches\n",
    "                if b == max_val_batches:\n",
    "                    break\n",
    "    \n",
    "                # Apply the model\n",
    "                y_pred = CNNmodel(X_val)\n",
    "    \n",
    "                # Tally the number of correct predictions\n",
    "                predicted = torch.max(y_pred.data, 1)[1]\n",
    "                val_corr += (predicted == y_val).sum()\n",
    "                \n",
    "                # Store predictions of final epoch for confusion matrix\n",
    "                if(i == epochs - 1):\n",
    "                    final_predicted += predicted\n",
    "                    final_actual += y_val\n",
    "    \n",
    "        loss = criterion(y_pred, y_val)\n",
    "        val_losses.append(loss)\n",
    "        val_correct.append(val_corr)\n",
    "        \n",
    "        \n",
    "        print(f'Overall train accuracy after {i}th epoch: {train_correct[i]*(100/len(train_data))}')\n",
    "        print(f'Overall validation accuracy after {i}th epoch: {val_correct[i]*(100/len(val_data))}')\n",
    "    print(confusion_matrix(final_actual, final_predicted))\n",
    "    print(classification_report(final_actual,final_predicted))\n",
    "    print(f'\\nDuration for training: {time.time() - start_time:.0f} seconds') # print the time elapsed for training\n",
    "    \n",
    "############################## Train with data and save model ############################\n",
    "\n",
    "# Train with data\n",
    "train(images, labels)\n",
    "\n",
    "# Save the model\n",
    "torch.save(CNNmodel.state_dict(), \"saved_model.pt\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NGC-PyTorch-1.9",
   "language": "python",
   "name": "ngc-pytorch-1.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
